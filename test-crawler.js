import axios from 'axios';
import cheerio from 'cheerio';

const BASE_URL = 'http://localhost:5173';

async function testCrawler() {
  console.log('üîç Testing crawler accessibility...\n');
  
  try {
    // Test homepage
    console.log('1. Testing homepage...');
    const homeResponse = await axios.get(`${BASE_URL}/`);
    const homeHtml = homeResponse.data;
    const $home = cheerio.load(homeHtml);
    
    // Check for internal links
    const internalLinks = [];
    $home('a[href^="/"]').each((i, el) => {
      const href = $home(el).attr('href');
      if (href && !href.includes('#')) {
        internalLinks.push(href);
      }
    });
    
    console.log(`‚úÖ Homepage loaded successfully`);
    console.log(`‚úÖ Found ${internalLinks.length} internal links:`);
    internalLinks.forEach(link => console.log(`   - ${link}`));
    
    // Test if we can access other pages
    const testPages = ['/about', '/contact', '/blogs'];
    
    for (const page of testPages) {
      try {
        console.log(`\n2. Testing ${page}...`);
        const response = await axios.get(`${BASE_URL}${page}`);
        const $page = cheerio.load(response.data);
        
        // Check for title
        const title = $page('title').text();
        console.log(`‚úÖ ${page} loaded successfully`);
        console.log(`‚úÖ Title: ${title}`);
        
        // Check for H1
        const h1 = $page('h1').text();
        console.log(`‚úÖ H1: ${h1}`);
        
      } catch (error) {
        console.log(`‚ùå Error accessing ${page}: ${error.message}`);
      }
    }
    
    // Test sitemap
    console.log('\n3. Testing sitemap...');
    try {
      const sitemapResponse = await axios.get(`${BASE_URL}/sitemap.xml`);
      console.log('‚úÖ Sitemap accessible');
      console.log(`‚úÖ Sitemap size: ${sitemapResponse.data.length} characters`);
    } catch (error) {
      console.log(`‚ùå Sitemap error: ${error.message}`);
    }
    
    // Test robots.txt
    console.log('\n4. Testing robots.txt...');
    try {
      const robotsResponse = await axios.get(`${BASE_URL}/robots.txt`);
      console.log('‚úÖ Robots.txt accessible');
      console.log(`‚úÖ Robots.txt content: ${robotsResponse.data.substring(0, 100)}...`);
    } catch (error) {
      console.log(`‚ùå Robots.txt error: ${error.message}`);
    }
    
    console.log('\nüéâ Crawler test completed!');
    console.log('\nüìã Summary:');
    console.log('- Internal links are now available for crawlers');
    console.log('- Sitemap.xml guides crawlers to all pages');
    console.log('- Robots.txt allows crawling');
    console.log('- All pages should now be discoverable by Screaming Frog');
    
  } catch (error) {
    console.error('‚ùå Test failed:', error.message);
  }
}

testCrawler();
